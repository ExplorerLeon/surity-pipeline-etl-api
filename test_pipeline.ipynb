{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5592f7e8",
   "metadata": {},
   "source": [
    "!!!**GO THROUGH TODO COMMENTS**!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103a1fb2-e13b-4ec8-83c3-55a55c923708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to set env variable\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569df303-5e14-4695-933b-ba9b60e40c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Google library authentication refer to markdown below\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/leonsmith/.config/gcloud/application_default_credentials.json\" # See readme to update\n",
    "\n",
    "## Use in future\n",
    "# str(Path('~').expanduser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07008e27-5f78-4e86-bdf9-ccf2c06d3391",
   "metadata": {},
   "source": [
    "User credentials\n",
    "When your code is running in a local development environment, such as a development workstation, the best option is to use credentials associated with your Google Account, also called user credentials.\n",
    "\n",
    "To provide your user credentials to ADC, you use the Google Cloud CLI:\n",
    "\n",
    "Install and initialize the gcloud CLI. <link https://cloud.google.com/sdk/docs/install>\n",
    "\n",
    "Create your credential file:\n",
    "\n",
    "\n",
    "gcloud auth application-default login\n",
    "A login screen is displayed. After you log in, your credentials are stored in the local credential file used by ADC.\n",
    "\n",
    "You can provide user credentials to ADC by running the gcloud auth application-default login command. This command places a JSON file containing the credentials you provide (usually from your own Google Account) in a well-known location on your file system. The location depends on your operating system:\n",
    "\n",
    "Linux, macOS: $HOME/.config/gcloud/application_default_credentials.json\n",
    "Windows: %APPDATA%\\gcloud\\application_default_credentials.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb7c59a-f918-4831-bc43-367d8c1010ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "\n",
    "PROJECT_ID = 'assetinsure-surety-data-models'\n",
    "BUCKET = 'surety-data-models'\n",
    "# TABLE_SPEC = f'{PROJECT_ID}:ls_panthers_test.panters-test-table-1'\n",
    "FILES_FOLDER = 'input'\n",
    "OUTPUT_FOLDER = 'output' # GCS_OUTPUT_FOLDER\n",
    "TABLE_NAME = \"book\"\n",
    "DATASET = \"ls_panthers_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e9226e-4e36-4d62-9752-8b637407d858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure we can see all packages in root of repo\n",
    "repo_root_dir = str(Path(\".\").resolve().parent)\n",
    "if repo_root_dir not in sys.path:\n",
    "    sys.path.append(repo_root_dir)\n",
    "\n",
    "sys.path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0953db-e5a6-4b0d-8173-124dc61debaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for local\n",
    "base_dir = Path(sys.path[0])\n",
    "\n",
    "temp_e_dir = base_dir / \"temp_e\"\n",
    "temp_e_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "temp_l_dir = base_dir / \"temp_l\"\n",
    "temp_l_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "logs_dir = base_dir / \"logs\"\n",
    "logs_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44df5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging function\n",
    "def pipeline_log(logs_dir, message, table_name):\n",
    "\n",
    "    with open(f\"{logs_dir}/{table_name}_logs.json\", \"a\") as f:\n",
    "        data = {\n",
    "            \"timestamp\": str(datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')),\n",
    "            \"logs\": message\n",
    "        }\n",
    "        \n",
    "        json.dump(data, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c544fd3d-1714-48d2-9afe-b2dc27dbe52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Instantiates clients\n",
    "# TODO: where to put in function or above\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56bb6d4-ccb8-4cf2-a160-8c5167ac333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_blobs(bucket_name, destination_directory, source_blob_prefix=\"\", file_pattern=\".*\\.xlsm\"):\n",
    "    \"\"\"Downloads all blobs from the bucket and saves them with their original file names.\"\"\"\n",
    "    # Initialise the Google Cloud Storage client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # List all blobs in the bucket with the specified prefix\n",
    "    blobs = list(bucket.list_blobs(prefix=source_blob_prefix))\n",
    "\n",
    "    local_file_paths = []\n",
    "    \n",
    "    for blob in blobs:\n",
    "        # Don't download subfolder items\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            # Skip directories\n",
    "            continue\n",
    "        \n",
    "        if re.search(file_pattern, blob.name.split(\"/\")[-1]):\n",
    "\n",
    "            # Extract file name\n",
    "            file_name = blob.name.split(\"/\")[-1]\n",
    "            \n",
    "            # Construct the local file path by joining the destination directory and relative path\n",
    "            local_file_path = os.path.join(destination_directory, file_name)\n",
    "            \n",
    "            # Download the blob to the destination with its original name\n",
    "            blob.download_to_filename(local_file_path)\n",
    "            \n",
    "            print(\"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
    "                blob.name, bucket_name, local_file_path))\n",
    "\n",
    "            local_file_paths.append(local_file_path)\n",
    "\n",
    "    return local_file_paths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c550b4a-606f-43f7-a92e-aeff92ef862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded storage object input/Book.xlsm from bucket surety-data-models to local file /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Book.xlsm.\n",
      "Downloaded storage object input/Panthers Financial Model Oct-22.xlsm from bucket surety-data-models to local file /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Panthers Financial Model Oct-22.xlsm.\n",
      "Downloaded storage object input/not_book.xlsm from bucket surety-data-models to local file /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/not_book.xlsm.\n"
     ]
    }
   ],
   "source": [
    "# Download the files from GS to local temp_e and store file paths\n",
    "raw_file_paths = download_all_blobs(BUCKET, temp_e_dir, FILES_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81dc8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"Loading these files locally: {raw_file_paths}\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb7f243-f292-455a-9ea1-f21b470062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel_files(file_path):\n",
    "    try:\n",
    "        # Attempt to read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        # Log the filename and the error message\n",
    "        print(f\"Error: {e}. File not found: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "442111a3-145f-4b30-8302-26365a11e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(df):\n",
    "    # Get company name\n",
    "    # first_non_null_index = df[df.iloc[:, 0].notnull()].index[0]\n",
    "    # company_name = df.iloc[first_non_null_index, 0]\n",
    "    # # Print company name\n",
    "    # print(company_name)\n",
    "\n",
    "    # # Get the current date and time\n",
    "    # current_time = datetime.datetime.now()\n",
    "    # # Print the current time\n",
    "    # print(current_time)\n",
    "\n",
    "    # # Create a dictionary to represent the row data\n",
    "    # row_data = {\n",
    "    #     'ID': [10,11,12],  # Replace with the actual ID if available\n",
    "    #     'CompanyName': [company_name,company_name,company_name],\n",
    "    #     'Date': [current_time,current_time,current_time]\n",
    "    #     }\n",
    "\n",
    "    # # Create dataframe of dictionary\n",
    "    # df = pd.DataFrame.from_dict(row_data, orient='index').T\n",
    "\n",
    "    # new test\n",
    "    df[\"timestamp\"] = datetime.datetime.now()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5b568f-c40d-4561-8152-bda916ee777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(paths):\n",
    "    dfs_transformed = []\n",
    "    \n",
    "    for file_path in raw_file_paths:\n",
    "        \n",
    "        df_r = read_excel_files(file_path)\n",
    "        \n",
    "        # Check if the file was successfully loaded\n",
    "        if df_r is not None:\n",
    "            # Process the DataFrame here\n",
    "            print(f\"File '{file_path}' loaded successfully.\")\n",
    "            \n",
    "            df_t = transformation(df_r)\n",
    "            dfs_transformed.append(df_t)\n",
    "        else:\n",
    "            # Continue processing or take other actions\n",
    "            print(f\"Skipping '{file_path}' due to the file not found error.\")\n",
    "        \n",
    "    df_comb_transf = pd.concat(dfs_transformed)\n",
    "    \n",
    "    return df_comb_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9efefe11-1a6f-40fd-a276-8d3f03b3fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Book.xlsm' loaded successfully.\n",
      "Error: [Errno 2] No such file or directory: '/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Panthers Financial Model Oct-22.xlsm'. File not found: /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Panthers Financial Model Oct-22.xlsm\n",
      "Skipping '/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Panthers Financial Model Oct-22.xlsm' due to the file not found error.\n",
      "File '/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/not_book.xlsm' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "df_bq = apply_transforms(raw_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf91d29-7bce-446d-8d71-2fea4da04360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>boolean</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>test</td>\n",
       "      <td>2023-08-02</td>\n",
       "      <td>200.45</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-09-27 12:59:58.594947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Real Test</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>3095452.85</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-09-27 12:59:58.594947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Final</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>287.11</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-09-27 12:59:58.594947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303</td>\n",
       "      <td>prod</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>82301.32</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-09-27 12:59:58.605081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>quick prod</td>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>399075.00</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-09-27 12:59:58.605081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305</td>\n",
       "      <td>yesterday</td>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>223.60</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-09-27 12:59:58.605081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number        name       date       value  boolean  \\\n",
       "0     101        test 2023-08-02      200.45    False   \n",
       "1     102   Real Test 2023-01-05  3095452.85    False   \n",
       "2     103       Final 2021-06-09      287.11     True   \n",
       "0     303        prod 2020-04-22    82301.32     True   \n",
       "1     304  quick prod 2016-01-09   399075.00     True   \n",
       "2     305   yesterday 2018-11-29      223.60     True   \n",
       "\n",
       "                   timestamp  \n",
       "0 2023-09-27 12:59:58.594947  \n",
       "1 2023-09-27 12:59:58.594947  \n",
       "2 2023-09-27 12:59:58.594947  \n",
       "0 2023-09-27 12:59:58.605081  \n",
       "1 2023-09-27 12:59:58.605081  \n",
       "2 2023-09-27 12:59:58.605081  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08f51431",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"Transformed data to df with shape: {df_bq.shape}\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16bd9c13-7034-4d61-87de-369243b3b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_local(df, path):\n",
    "    df.to_parquet(f\"{path}/{TABLE_NAME}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b51d46a4-6bb1-4089-9c70-e462521d9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_local(df_bq, temp_l_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08d71b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"DF to local: {temp_l_dir}/{TABLE_NAME}.parquet\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46110ca2-b075-4a51-81ad-65207cd2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    \n",
    "    # TODO: set the version in json config - to ovewrite pass None\n",
    "    generation_match_precondition = None\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4442f1c6-493f-4b64-93dd-9d79ce2df3a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_l/book.parquet uploaded to output/book.parquet.\n"
     ]
    }
   ],
   "source": [
    "upload_blob(BUCKET, f\"{temp_l_dir}/{TABLE_NAME}.parquet\", f\"{OUTPUT_FOLDER}/{TABLE_NAME}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81e6111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"DF to GS: gs//{BUCKET}/{OUTPUT_FOLDER}/{TABLE_NAME}.parquet\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e72f96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_to_bigquery(project_id, bucket_name, output_folder, dataset_id, table_id):\n",
    "    # Initialise BigQuery and Storage clients\n",
    "    bigquery_client = bigquery.Client()\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Construct transformed parquet file source URI of Google Storage object \n",
    "    source_uri = f\"gs://{bucket_name}/{output_folder}/{table_id}.parquet\"\n",
    "    \n",
    "    # Construct BigQuery table reference\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    \n",
    "    # Load job config details\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Options: WRITE_APPEND, WRITE_TRUNCATE, WRITE_EMPTY\n",
    "        source_format=bigquery.SourceFormat.PARQUET        \n",
    "        \n",
    "    )\n",
    "    \n",
    "    # Start the job to load from GS to BQ\n",
    "    load_job = bigquery_client.load_table_from_uri(\n",
    "        source_uri, table_id, job_config=job_config    \n",
    "    )\n",
    "    \n",
    "    print(f\"Load data from GS location: {source_uri} to BQ Table ID: {load_job.destination}\")\n",
    "    \n",
    "    # Wait for job to complete\n",
    "    load_job.result()\n",
    "    \n",
    "    # Print job status\n",
    "    # TODO: set logging\n",
    "    print(f\"Job ID: {load_job.job_id}\")\n",
    "    print(f\"Job State: {load_job.state}\")\n",
    "    print(f\"Loaded {load_job.output_rows} rows into {dataset_id}.{table_id} from {source_uri}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "515cc2f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/env/lib/python3.9/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from GS location: gs://surety-data-models/output/book.parquet to BQ Table ID: assetinsure-surety-data-models.ls_panthers_test.book\n",
      "Job ID: 4ad1ab7f-8ac7-4563-b7db-a8f5a2c68874\n",
      "Job State: DONE\n",
      "Loaded 6 rows into ls_panthers_test.assetinsure-surety-data-models.ls_panthers_test.book from gs://surety-data-models/output/book.parquet\n"
     ]
    }
   ],
   "source": [
    "pipeline_job = load_parquet_to_bigquery(PROJECT_ID, BUCKET, OUTPUT_FOLDER, DATASET, TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73b6341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"Wrote to BQ successfully: {PROJECT_ID}.{DATASET}.{TABLE_NAME}\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e930a3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: cleanup state include in script to check to continue to cleanup\n",
    "pipeline_job.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ae042a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(clean_directory):\n",
    "    # List all files to delete\n",
    "    files = [f for f in clean_directory.glob(\"*\")]\n",
    "    for f in files:\n",
    "        print(f\"Deleting: {f}\")\n",
    "    \n",
    "    try:\n",
    "        rmtree(clean_directory)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "260afae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"PIPELINE RAN SUCCESSFULLY - DELETE TEMP FILES IN DIRS {temp_e_dir} and {temp_l_dir}: \", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5ed735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting: /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/not_book.xlsm\n",
      "Deleting: /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/.DS_Store\n",
      "Deleting: /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_e/Book.xlsm\n",
      "Deleting: /Users/leonsmith/explore/fs/assetinsure/surity_pipeline/surity-pipeline-etl-api/temp_l/book.parquet\n"
     ]
    }
   ],
   "source": [
    "# List directories to clean\n",
    "for path in [temp_e_dir, temp_l_dir]:\n",
    "    cleanup(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10905ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log(logs_dir, f\"PIPELINE END\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17219071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surity-api-pipeline",
   "language": "python",
   "name": "surity-api-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
